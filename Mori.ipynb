{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkmAT1IMchQq",
        "outputId": "80396efd-e536-4576-895b-2b4b0d2b4ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'semantic-search-engine'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 12 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 2.00 MiB | 12.38 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Allliance/semantic-search-engine\n",
        "%cd semantic-search-engine/data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9RMHWOifE86",
        "outputId": "233dc11e-ec5d-405e-b143-9c1f40cf04d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.47.1)\n",
            "Collecting clip-by-openai (from -r requirements.txt (line 3))\n",
            "  Downloading clip_by_openai-1.1-py3-none-any.whl.metadata (369 bytes)\n",
            "Collecting pinecone-client (from -r requirements.txt (line 4))\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (4.67.1)\n",
            "Collecting ftfy (from clip-by-openai->-r requirements.txt (line 3))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "INFO: pip is looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting clip-by-openai (from -r requirements.txt (line 3))\n",
            "  Downloading clip_by_openai-1.0.1-py3-none-any.whl.metadata (407 bytes)\n",
            "  Downloading clip_by_openai-0.1.1.5-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading clip_by_openai-0.1.1.4-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading clip_by_openai-0.1.1.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Downloading clip_by_openai-0.1.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "  Downloading clip_by_openai-0.1.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "  Downloading clip_by_openai-0.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "INFO: pip is still looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install -r requirements.txt (line 3) and torch because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested torch\n",
            "    clip-by-openai 1.1 depends on torch<1.7.2 and >=1.7.1\n",
            "    The user requested torch\n",
            "    clip-by-openai 1.0.1 depends on torch<1.7.2 and >=1.7.1\n",
            "    The user requested torch\n",
            "    clip-by-openai 0.1.1.5 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    clip-by-openai 0.1.1.4 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    clip-by-openai 0.1.1.3 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    clip-by-openai 0.1.1.2 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    clip-by-openai 0.1.1 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    clip-by-openai 0.1.0 depends on torch==1.7.1\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL3oZcemfVmX",
        "outputId": "e42fa07b-2745-49fa-fc6a-75d81f4c94bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone-client\n",
            "  Using cached pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.3.0)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
            "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "# from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "def create_index(index_name=\"products-index\",\n",
        "                 dimension=512):\n",
        "\n",
        "\n",
        "    pc = Pinecone(\n",
        "            api_key=\"pcsk_5q2j4m_CKKRfFmwKj24tNiiQ6d3uq2bWxqVSYHHWaCRbnSi2VUCc3WDJZQgRHN5Jk2kfSg\"\n",
        "        )\n",
        "\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        pc.create_index(index_name,\n",
        "                        dimension=dimension,\n",
        "                        metric=\"cosine\",\n",
        "                        spec=ServerlessSpec(\n",
        "                        cloud='aws',\n",
        "                        region='us-east-1',\n",
        "                        ))\n",
        "\n",
        "    return pc"
      ],
      "metadata": {
        "id": "Ois999z4f9JE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode_text_with_processor(text):\n",
        "    # Preprocess the text using CLIPProcessor\n",
        "    inputs = preprocess(text=text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Send inputs to device (CUDA or CPU)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Get the text embedding from the CLIP model\n",
        "    with torch.no_grad():\n",
        "        text_features = model.get_text_features(**inputs)\n",
        "\n",
        "    # Normalize the text features\n",
        "    text_features /= text_features.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "    return text_features"
      ],
      "metadata": {
        "id": "uWCQXoM3mfR7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # type: ignore\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "index_name = \"test-index\"\n",
        "host = \"https://test-index-cw4v01q.svc.aped-4627-b74a.pinecone.io\"\n",
        "pc = create_index(index_name)\n",
        "# Load test images and their embeddings\n",
        "\n",
        "\n",
        "def load_image_and_embedding(image_paths):\n",
        "    images = [Image.open(image_path) for image_path in image_paths]\n",
        "    # image = Image.open(image_path)\n",
        "    inputs = preprocess(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(**inputs)\n",
        "\n",
        "    # Normalize the image features\n",
        "    image_features /= image_features.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "    return image_features, inputs['pixel_values']\n",
        "\n",
        "def encode_text_with_processor(text):\n",
        "    # Preprocess the text using CLIPProcessor\n",
        "    inputs = preprocess(text=text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Send inputs to device (CUDA or CPU)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Get the text embedding from the CLIP model\n",
        "    with torch.no_grad():\n",
        "        text_features = model.get_text_features(**inputs)\n",
        "\n",
        "    # Normalize the text features\n",
        "    text_features /= text_features.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "    return text_features\n",
        "\n",
        "# show the image using plt\n",
        "def show_image(image):\n",
        "    image = image.squeeze(0)\n",
        "    image = image.permute(1, 2, 0)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def upsert_embeddings(product_ids, embeddings):\n",
        "    # Convert embeddings to a list of tuples (id, vector)\n",
        "    vectors = [{\n",
        "        \"id\": str(product_id),\n",
        "        \"values\": embedding.tolist(),\n",
        "        \"metadata\": {},\n",
        "        } for product_id, embedding in zip(product_ids, embeddings)]\n",
        "\n",
        "    # Connect to the index\n",
        "    index = pc.Index(index_name)#, host=host)\n",
        "\n",
        "    # Upsert the embeddings\n",
        "    index.upsert(vectors)\n",
        "\n",
        "image_files = {\n",
        "    'cat': 'test_images/cat.png',\n",
        "    'dog': 'test_images/dog.png',\n",
        "    'elephant': 'test_images/elephant.jpg',\n",
        "}\n",
        "\n",
        "names = image_files.keys()\n",
        "image_paths = [image_files[name] for name in names]\n",
        "\n",
        "embeddings, images = load_image_and_embedding(image_paths)\n",
        "\n",
        "print(len(embeddings.tolist()))\n",
        "# print(image.size())\n",
        "# show_image(image)\n",
        "\n",
        "# Upsert the image embedding\n",
        "upsert_embeddings(names, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh-sl9hWfUWH",
        "outputId": "992d29f3-07da-4007-da40-7bfca5f2a7b8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"persian\"\n",
        "query_embedding = encode_text_with_processor(query).squeeze(0)\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "response = index.query(\n",
        "    vector=query_embedding.tolist(),\n",
        "    top_k=1,\n",
        "    include_values=True,\n",
        "    # include_metadata=True,\n",
        "    # filter={\"genre\": {\"$eq\": \"action\"}}\n",
        ")\n",
        "\n",
        "print(response['matches'][0]['id'], response['matches'][0]['score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-BZVxIEi0g1",
        "outputId": "c5ff2d88-ef50-4d9b-8cd1-4b8567c9e276"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat 0.235475332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnEwMDlimh1p",
        "outputId": "62961ba9-912f-4a4b-ecff-401fd2fd2cd3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}